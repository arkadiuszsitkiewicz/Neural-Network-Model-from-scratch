# Neural-Network-Model-from-scratch (using only numpy)
- activation functions: ReLU, tanh, sigmoid, softmax.
- regularization methods: L2 and dropout.
- optimalization methods: momentum, rms_prop, adam, learning rate decay
- classification methods: binary and multi-class


This project was growing through time, hance in each notebook, I present some functionalities, that in the end sum up to powerfull NN model. Overall analysis and examples are presented in notebooks:
- notebook #01 - presenting simple NN model with ReLU, tanh, sigmoid activation function
- notebook #02 - presenting L2 and dropout regularization techniques
- notebook #03 - presenting optimalization methods: momentum, rms_prop, adam, learning rate decay
- notebook #04 - presenting multiclass classification and explaining math behind derivative of softmax as well as vectorization of it
